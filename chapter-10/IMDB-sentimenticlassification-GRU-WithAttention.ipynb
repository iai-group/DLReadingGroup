{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#os.environ['KERAS_BACKEND']='tensorflow'\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "os.environ['THEANO_FLAGS']='device=cuda0,floatX=float32,gpuarray.preallocate=0.3'\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id  sentiment                                             review\n",
      "0      5814_8          1  With all this stuff going down at the moment w...\n",
      "1      2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2      7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3      3630_4          0  It must be assumed that those who praised this...\n",
      "4      9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
      "5      8196_8          1  I dont know why people think this is such a ba...\n",
      "6      7166_2          0  This movie could have been very good, but come...\n",
      "7     10633_1          0  I watched this video at a friend's house. I'm ...\n",
      "8       319_1          0  A friend of mine bought this film for £1, and ...\n",
      "9     8713_10          1  <br /><br />This movie is full of references. ...\n",
      "10     2486_3          0  What happens when an army of wetbacks, towelhe...\n",
      "11    6811_10          1  Although I generally do not like remakes belie...\n",
      "12    11744_9          1  \\Mr. Harvey Lights a Candle\\\" is anchored by a...\n",
      "13     7369_1          0  I had a feeling that after \\Submerged\\\", this ...\n",
      "14    12081_1          0  note to George Litman, and others: the Mystery...\n",
      "15     3561_4          0  Stephen King adaptation (scripted by King hims...\n",
      "16     4489_1          0  `The Matrix' was an exciting summer blockbuste...\n",
      "17     3951_2          0  Ulli Lommel's 1980 film 'The Boogey Man' is no...\n",
      "18    3304_10          1  This movie is one among the very few Indian mo...\n",
      "19    9352_10          1  Most people, especially young people, may not ...\n",
      "20     3374_7          1  \\Soylent Green\\\" is one of the best and most d...\n",
      "21    10782_7          1  Michael Stearns plays Mike, a sexually frustra...\n",
      "22    5414_10          1  This happy-go-luck 1939 military swashbuckler,...\n",
      "23    10492_1          0  I would love to have that two hours of my life...\n",
      "24     3350_3          0  The script for this movie was probably found i...\n",
      "25     6581_7          1  Looking for Quo Vadis at my local video store,...\n",
      "26     2203_3          0  Note to all mad scientists everywhere: if you'...\n",
      "27      689_1          0  What the ........... is this ? This must, with...\n",
      "28     9152_1          0  Intrigued by the synopsis (every gay video the...\n",
      "29     6077_1          0  Would anyone really watch this RUBBISH if it d...\n",
      "..        ...        ...                                                ...\n",
      "969    5441_1          0  Of all the films I have seen, this one, The Ra...\n",
      "970    1712_9          1  This movie was disaster at Box Office, and the...\n",
      "971    1907_7          1  8 Simple Rules is a funny show but it also has...\n",
      "972    2394_3          0  This movie was strange... I watched it while i...\n",
      "973    8109_2          0  *** Contains Spoilers ***<br /><br />I did not...\n",
      "974   10040_2          0  ...for this movie defines a new low in Bollywo...\n",
      "975    1327_4          0  This dreadful film assembles every Asian stere...\n",
      "976   11545_8          1  The original movie, The Odd Couple, has some w...\n",
      "977  10747_10          1  Gédéon and Jules Naudet wanted to film a docum...\n",
      "978   12295_9          1  \\Hotel du Nord \\\" is the only Carné movie from...\n",
      "979    4367_4          0  One of Boris Karloff's real clinkers. Essentia...\n",
      "980     120_8          1  It is not every film's job to stimulate you su...\n",
      "981    3223_3          0  LOL! Not a bad way to start it. I thought this...\n",
      "982    1098_9          1  Modern viewers know this little film primarily...\n",
      "983    4815_2          0  Like most comments I saw this film under the n...\n",
      "984    1056_3          0  Diana Guzman is an angry young woman. Survivin...\n",
      "985    4588_9          1  Rated PG-13 for violence, brief sexual humor a...\n",
      "986    2701_1          0  First of all yes I'm white, so I try to tread ...\n",
      "987    3009_8          1  A film that is so much a 30's Warners film in ...\n",
      "988   12318_9          1  Though I'm not the biggest fan of wirework bas...\n",
      "989   4428_10          1  I have to totally disagree with the other comm...\n",
      "990    4180_3          0  I picked this movie up to replace the dismal c...\n",
      "991   12200_4          0  Usually, any film with Sylvester Stallone is u...\n",
      "992  10323_10          1  This is a VERY entertaining movie. A few of th...\n",
      "993     839_7          1  Think Pierce Brosnan and you think suave, dapp...\n",
      "994   11445_7          1  A new way to enjoy Goldsworthy's work, Rivers ...\n",
      "995     472_1          0  The only thing I remember about this movie are...\n",
      "996    1713_8          1  This is a kind of movie that will stay with yo...\n",
      "997     723_2          0  I just didn't get this movie...Was it a musica...\n",
      "998   10126_2          0  Granting the budget and time constraints of se...\n",
      "\n",
      "[999 rows x 3 columns]\n",
      "(999, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansatt/vsetty/miniconda2/envs/keras-conda-27/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/ansatt/vsetty/miniconda2/envs/keras-conda-27/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18949 unique tokens.\n",
      "('Shape of data tensor:', (999, 1000))\n",
      "('Shape of label tensor:', (999, 2))\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "tf.device('/gpu:0')\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "data = pd.read_csv('labeledTrainData-sample1000.tsv', sep='\\t')\n",
    "counts = data['sentiment'].value_counts()\n",
    "data_train = data[data['sentiment'].isin(counts[counts >= 20].index)]\n",
    "print(data_train)\n",
    "print(data_train.shape)\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in data_train.iterrows():\n",
    "    text = BeautifulSoup(data_train.review[idx])\n",
    "    texts.append(clean_str(text.get_text().encode('ascii','ignore')))\n",
    "    labels.append(data_train.sentiment[idx])\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing and validation set number of positive and negative reviews\n",
      "[404. 396.]\n",
      "[113.  86.]\n",
      "Total 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Traing and validation set number of positive and negative reviews')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "\n",
    "GLOVE_DIR = \"glove/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.840B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attention GRU network\t\t  \n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention GRU network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1000, 300)     5685000     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 1000, 600)     1081800     embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_2 (AttLayer)            (None, 600)           600         bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             1202        attlayer_2[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,768,602\n",
      "Trainable params: 6,768,602\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 800 samples, validate on 199 samples\n",
      "Epoch 1/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 1.0849 - acc: 0.4960Epoch 00000: loss improved from inf to 1.08093, saving model to weights-imdb-bigruatt-00-1.0809.hdf5\n",
      "800/800 [==============================] - 194s - loss: 1.0809 - acc: 0.4937 - val_loss: 0.7064 - val_acc: 0.5779\n",
      "Epoch 2/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 0.7327 - acc: 0.5213Epoch 00001: loss improved from 1.08093 to 0.72776, saving model to weights-imdb-bigruatt-01-0.7278.hdf5\n",
      "800/800 [==============================] - 194s - loss: 0.7278 - acc: 0.5263 - val_loss: 0.7264 - val_acc: 0.4322\n",
      "Epoch 3/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 0.6564 - acc: 0.6027Epoch 00002: loss improved from 0.72776 to 0.65643, saving model to weights-imdb-bigruatt-02-0.6564.hdf5\n",
      "800/800 [==============================] - 195s - loss: 0.6564 - acc: 0.6013 - val_loss: 0.6748 - val_acc: 0.5377\n",
      "Epoch 4/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 0.5830 - acc: 0.6867Epoch 00003: loss improved from 0.65643 to 0.57637, saving model to weights-imdb-bigruatt-03-0.5764.hdf5\n",
      "800/800 [==============================] - 196s - loss: 0.5764 - acc: 0.6913 - val_loss: 0.5693 - val_acc: 0.7136\n",
      "Epoch 5/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 0.4787 - acc: 0.7347Epoch 00004: loss improved from 0.57637 to 0.48175, saving model to weights-imdb-bigruatt-04-0.4817.hdf5\n",
      "800/800 [==============================] - 196s - loss: 0.4817 - acc: 0.7338 - val_loss: 0.4022 - val_acc: 0.8342\n",
      "Epoch 6/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 0.2609 - acc: 0.8920Epoch 00005: loss improved from 0.48175 to 0.25805, saving model to weights-imdb-bigruatt-05-0.2581.hdf5\n",
      "800/800 [==============================] - 196s - loss: 0.2581 - acc: 0.8925 - val_loss: 0.3694 - val_acc: 0.8593\n",
      "Epoch 7/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 0.1542 - acc: 0.9360Epoch 00006: loss improved from 0.25805 to 0.15169, saving model to weights-imdb-bigruatt-06-0.1517.hdf5\n",
      "800/800 [==============================] - 194s - loss: 0.1517 - acc: 0.9388 - val_loss: 0.6049 - val_acc: 0.8291\n",
      "Epoch 8/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 0.0748 - acc: 0.9773Epoch 00007: loss improved from 0.15169 to 0.07247, saving model to weights-imdb-bigruatt-07-0.0725.hdf5\n",
      "800/800 [==============================] - 193s - loss: 0.0725 - acc: 0.9788 - val_loss: 1.0616 - val_acc: 0.7940\n",
      "Epoch 9/20\n",
      "750/800 [===========================>..] - ETA: 11s - loss: 0.1410 - acc: 0.9520Epoch 00008: loss did not improve\n",
      "800/800 [==============================] - 188s - loss: 0.1331 - acc: 0.9550 - val_loss: 0.5307 - val_acc: 0.8392\n",
      "Epoch 10/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 0.0075 - acc: 0.9987Epoch 00009: loss improved from 0.07247 to 0.03798, saving model to weights-imdb-bigruatt-09-0.0380.hdf5\n",
      "800/800 [==============================] - 181s - loss: 0.0380 - acc: 0.9913 - val_loss: 1.2524 - val_acc: 0.7136\n",
      "Epoch 11/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 0.0424 - acc: 0.9867Epoch 00010: loss did not improve\n",
      "800/800 [==============================] - 180s - loss: 0.0400 - acc: 0.9875 - val_loss: 0.5410 - val_acc: 0.8643\n",
      "Epoch 12/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 0.0024 - acc: 1.0000Epoch 00011: loss improved from 0.03798 to 0.00230, saving model to weights-imdb-bigruatt-11-0.0023.hdf5\n",
      "800/800 [==============================] - 180s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.6569 - val_acc: 0.8693\n",
      "Epoch 13/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 5.4004e-04 - acc: 1.0000Epoch 00012: loss improved from 0.00230 to 0.00053, saving model to weights-imdb-bigruatt-12-0.0005.hdf5\n",
      "800/800 [==============================] - 182s - loss: 5.2672e-04 - acc: 1.0000 - val_loss: 0.8094 - val_acc: 0.8593\n",
      "Epoch 14/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 1.5510e-04 - acc: 1.0000Epoch 00013: loss improved from 0.00053 to 0.00015, saving model to weights-imdb-bigruatt-13-0.0002.hdf5\n",
      "800/800 [==============================] - 182s - loss: 1.5217e-04 - acc: 1.0000 - val_loss: 0.9083 - val_acc: 0.8593\n",
      "Epoch 15/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 5.8800e-05 - acc: 1.0000Epoch 00014: loss improved from 0.00015 to 0.00006, saving model to weights-imdb-bigruatt-14-0.0001.hdf5\n",
      "800/800 [==============================] - 182s - loss: 5.5889e-05 - acc: 1.0000 - val_loss: 1.0228 - val_acc: 0.8543\n",
      "Epoch 16/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 2.0773e-05 - acc: 1.0000Epoch 00015: loss improved from 0.00006 to 0.00002, saving model to weights-imdb-bigruatt-15-0.0000.hdf5\n",
      "800/800 [==============================] - 182s - loss: 2.0554e-05 - acc: 1.0000 - val_loss: 1.1900 - val_acc: 0.8492\n",
      "Epoch 17/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 7.7361e-06 - acc: 1.0000Epoch 00016: loss improved from 0.00002 to 0.00001, saving model to weights-imdb-bigruatt-16-0.0000.hdf5\n",
      "800/800 [==============================] - 181s - loss: 7.6508e-06 - acc: 1.0000 - val_loss: 1.3682 - val_acc: 0.8442\n",
      "Epoch 18/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 2.8332e-06 - acc: 1.0000Epoch 00017: loss improved from 0.00001 to 0.00000, saving model to weights-imdb-bigruatt-17-0.0000.hdf5\n",
      "800/800 [==============================] - 185s - loss: 2.8159e-06 - acc: 1.0000 - val_loss: 1.5124 - val_acc: 0.8442\n",
      "Epoch 19/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 0.3167 - acc: 0.9653Epoch 00018: loss did not improve\n",
      "800/800 [==============================] - 185s - loss: 0.2971 - acc: 0.9675 - val_loss: 0.6346 - val_acc: 0.8342\n",
      "Epoch 20/20\n",
      "750/800 [===========================>..] - ETA: 10s - loss: 0.0021 - acc: 1.0000Epoch 00019: loss did not improve\n",
      "800/800 [==============================] - 184s - loss: 0.0020 - acc: 1.0000 - val_loss: 0.6534 - val_acc: 0.8593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09ed88f0d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_gru = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer()(l_gru)\n",
    "preds = Dense(2, activation='softmax')(l_att)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention GRU network\")\n",
    "model.summary()\n",
    "filepath=\"weights-imdb-bigruatt-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=20, batch_size=50, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
