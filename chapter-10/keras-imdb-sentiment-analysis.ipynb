{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ansatt/vsetty/miniconda2/envs/keras-conda-27/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.GeneratorContextManager at 0x7fdd7369cf10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "os.environ['THEANO_FLAGS']='device=cuda0,floatX=float32'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "tf.Session(config=config)\n",
    "tf.device('/gpu:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansatt/vsetty/miniconda2/envs/keras-conda-27/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/ansatt/vsetty/miniconda2/envs/keras-conda-27/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18949 unique tokens.\n",
      "('Shape of data tensor:', (999, 1000))\n",
      "('Shape of label tensor:', (999, 2))\n",
      "Traing and validation set number of positive and negative reviews\n",
      "[406. 394.]\n",
      "[111.  88.]\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('labeledTrainData-sample1000.tsv', sep='\\t')\n",
    "print data_train.shape\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(data_train.review.shape[0]):\n",
    "    text = BeautifulSoup(data_train.review[idx])\n",
    "    texts.append(clean_str(text.get_text().encode('ascii','ignore')))\n",
    "    labels.append(data_train.sentiment[idx])\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Traing and validation set number of positive and negative reviews')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"glove/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Bidirectional LSTM\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1000, 100)     1895000     input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional)  (None, 200)           160800      embedding_3[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             402         bidirectional_4[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 2,056,202\n",
      "Trainable params: 2,056,202\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 800 samples, validate on 199 samples\n",
      "Epoch 1/20\n",
      "750/800 [===========================>..] - ETA: 4s - loss: 0.7565 - acc: 0.5013Epoch 00000: loss improved from inf to 0.75496, saving model to weights-imdb-bilstm-00-0.7550.hdf5\n",
      "800/800 [==============================] - 70s - loss: 0.7550 - acc: 0.4988 - val_loss: 0.6948 - val_acc: 0.5126\n",
      "Epoch 2/20\n",
      "750/800 [===========================>..] - ETA: 4s - loss: 0.6669 - acc: 0.6053Epoch 00001: loss improved from 0.75496 to 0.66087, saving model to weights-imdb-bilstm-01-0.6609.hdf5\n",
      "800/800 [==============================] - 83s - loss: 0.6609 - acc: 0.6112 - val_loss: 0.7177 - val_acc: 0.5578\n",
      "Epoch 3/20\n",
      "750/800 [===========================>..] - ETA: 4s - loss: 0.6207 - acc: 0.6507Epoch 00002: loss improved from 0.66087 to 0.61670, saving model to weights-imdb-bilstm-02-0.6167.hdf5\n",
      "800/800 [==============================] - 83s - loss: 0.6167 - acc: 0.6550 - val_loss: 0.6199 - val_acc: 0.6533\n",
      "Epoch 4/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.5556 - acc: 0.7147 Epoch 00003: loss improved from 0.61670 to 0.56247, saving model to weights-imdb-bilstm-03-0.5625.hdf5\n",
      "800/800 [==============================] - 92s - loss: 0.5625 - acc: 0.7088 - val_loss: 0.7485 - val_acc: 0.5930\n",
      "Epoch 5/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.5147 - acc: 0.7533 Epoch 00004: loss improved from 0.56247 to 0.50968, saving model to weights-imdb-bilstm-04-0.5097.hdf5\n",
      "800/800 [==============================] - 93s - loss: 0.5097 - acc: 0.7575 - val_loss: 0.5929 - val_acc: 0.6985\n",
      "Epoch 6/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.4401 - acc: 0.8093 Epoch 00005: loss improved from 0.50968 to 0.44263, saving model to weights-imdb-bilstm-05-0.4426.hdf5\n",
      "800/800 [==============================] - 90s - loss: 0.4426 - acc: 0.8012 - val_loss: 0.6017 - val_acc: 0.6482\n",
      "Epoch 7/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.3642 - acc: 0.8533 Epoch 00006: loss improved from 0.44263 to 0.36322, saving model to weights-imdb-bilstm-06-0.3632.hdf5\n",
      "800/800 [==============================] - 91s - loss: 0.3632 - acc: 0.8512 - val_loss: 0.7302 - val_acc: 0.6583\n",
      "Epoch 8/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.3423 - acc: 0.8627 Epoch 00007: loss improved from 0.36322 to 0.34777, saving model to weights-imdb-bilstm-07-0.3478.hdf5\n",
      "800/800 [==============================] - 86s - loss: 0.3478 - acc: 0.8537 - val_loss: 0.7033 - val_acc: 0.6382\n",
      "Epoch 9/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.2156 - acc: 0.9307 Epoch 00008: loss improved from 0.34777 to 0.21720, saving model to weights-imdb-bilstm-08-0.2172.hdf5\n",
      "800/800 [==============================] - 84s - loss: 0.2172 - acc: 0.9275 - val_loss: 0.6343 - val_acc: 0.6834\n",
      "Epoch 10/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.2086 - acc: 0.9307 Epoch 00009: loss improved from 0.21720 to 0.20274, saving model to weights-imdb-bilstm-09-0.2027.hdf5\n",
      "800/800 [==============================] - 90s - loss: 0.2027 - acc: 0.9337 - val_loss: 0.6267 - val_acc: 0.7136\n",
      "Epoch 11/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.1614 - acc: 0.9440 Epoch 00010: loss improved from 0.20274 to 0.16089, saving model to weights-imdb-bilstm-10-0.1609.hdf5\n",
      "800/800 [==============================] - 95s - loss: 0.1609 - acc: 0.9438 - val_loss: 0.7090 - val_acc: 0.6734\n",
      "Epoch 12/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0818 - acc: 0.9840 Epoch 00011: loss improved from 0.16089 to 0.07903, saving model to weights-imdb-bilstm-11-0.0790.hdf5\n",
      "800/800 [==============================] - 90s - loss: 0.0790 - acc: 0.9850 - val_loss: 0.7465 - val_acc: 0.6884\n",
      "Epoch 13/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0762 - acc: 0.9800 Epoch 00012: loss improved from 0.07903 to 0.07372, saving model to weights-imdb-bilstm-12-0.0737.hdf5\n",
      "800/800 [==============================] - 94s - loss: 0.0737 - acc: 0.9813 - val_loss: 0.7782 - val_acc: 0.6834\n",
      "Epoch 14/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0730 - acc: 0.9760 Epoch 00013: loss improved from 0.07372 to 0.07026, saving model to weights-imdb-bilstm-13-0.0703.hdf5\n",
      "800/800 [==============================] - 91s - loss: 0.0703 - acc: 0.9775 - val_loss: 0.8700 - val_acc: 0.6482\n",
      "Epoch 15/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0168 - acc: 0.9987 Epoch 00014: loss improved from 0.07026 to 0.01619, saving model to weights-imdb-bilstm-14-0.0162.hdf5\n",
      "800/800 [==============================] - 87s - loss: 0.0162 - acc: 0.9988 - val_loss: 0.9165 - val_acc: 0.6935\n",
      "Epoch 16/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0071 - acc: 0.9987 Epoch 00015: loss improved from 0.01619 to 0.00967, saving model to weights-imdb-bilstm-15-0.0097.hdf5\n",
      "800/800 [==============================] - 84s - loss: 0.0097 - acc: 0.9975 - val_loss: 2.1415 - val_acc: 0.6131\n",
      "Epoch 17/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0981 - acc: 0.9773 Epoch 00016: loss did not improve\n",
      "800/800 [==============================] - 86s - loss: 0.0927 - acc: 0.9788 - val_loss: 0.8661 - val_acc: 0.6884\n",
      "Epoch 18/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0043 - acc: 1.0000 Epoch 00017: loss improved from 0.00967 to 0.00420, saving model to weights-imdb-bilstm-17-0.0042.hdf5\n",
      "800/800 [==============================] - 88s - loss: 0.0042 - acc: 1.0000 - val_loss: 0.9722 - val_acc: 0.6834\n",
      "Epoch 19/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0019 - acc: 1.0000 Epoch 00018: loss improved from 0.00420 to 0.00193, saving model to weights-imdb-bilstm-18-0.0019.hdf5\n",
      "800/800 [==============================] - 85s - loss: 0.0019 - acc: 1.0000 - val_loss: 1.0327 - val_acc: 0.7136\n",
      "Epoch 20/20\n",
      "750/800 [===========================>..] - ETA: 5s - loss: 0.0346 - acc: 0.9853 Epoch 00019: loss did not improve\n",
      "800/800 [==============================] - 91s - loss: 0.0496 - acc: 0.9762 - val_loss: 0.9157 - val_acc: 0.6633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd615507ed0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "preds = Dense(2, activation='softmax')(l_lstm)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "model.summary()\n",
    "filepath=\"weights-imdb-bilstm-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=20, batch_size=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Bidirectional LSTM\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1000, 100)     1895000     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 200)           120600      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             402         bidirectional_2[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 2,016,002\n",
      "Trainable params: 2,016,002\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 800 samples, validate on 199 samples\n",
      "Epoch 1/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.8085 - acc: 0.5053Epoch 00000: loss improved from inf to 0.80484, saving model to weights-imdb-bilstm-00-0.8048.hdf5\n",
      "800/800 [==============================] - 55s - loss: 0.8048 - acc: 0.5025 - val_loss: 0.7049 - val_acc: 0.5578\n",
      "Epoch 2/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.6907 - acc: 0.5960Epoch 00001: loss improved from 0.80484 to 0.68615, saving model to weights-imdb-bilstm-01-0.6862.hdf5\n",
      "800/800 [==============================] - 59s - loss: 0.6862 - acc: 0.5988 - val_loss: 0.6969 - val_acc: 0.5176\n",
      "Epoch 3/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.6313 - acc: 0.6440Epoch 00002: loss improved from 0.68615 to 0.62623, saving model to weights-imdb-bilstm-02-0.6262.hdf5\n",
      "800/800 [==============================] - 57s - loss: 0.6262 - acc: 0.6463 - val_loss: 0.6781 - val_acc: 0.5528\n",
      "Epoch 4/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.6001 - acc: 0.6667Epoch 00003: loss improved from 0.62623 to 0.59581, saving model to weights-imdb-bilstm-03-0.5958.hdf5\n",
      "800/800 [==============================] - 59s - loss: 0.5958 - acc: 0.6713 - val_loss: 0.6247 - val_acc: 0.6533\n",
      "Epoch 5/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.5024 - acc: 0.7560Epoch 00004: loss improved from 0.59581 to 0.49935, saving model to weights-imdb-bilstm-04-0.4993.hdf5\n",
      "800/800 [==============================] - 57s - loss: 0.4993 - acc: 0.7625 - val_loss: 0.6138 - val_acc: 0.6583\n",
      "Epoch 6/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.4016 - acc: 0.8240Epoch 00005: loss improved from 0.49935 to 0.39722, saving model to weights-imdb-bilstm-05-0.3972.hdf5\n",
      "800/800 [==============================] - 59s - loss: 0.3972 - acc: 0.8287 - val_loss: 0.5934 - val_acc: 0.6784\n",
      "Epoch 7/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.3555 - acc: 0.8547Epoch 00006: loss improved from 0.39722 to 0.34724, saving model to weights-imdb-bilstm-06-0.3472.hdf5\n",
      "800/800 [==============================] - 55s - loss: 0.3472 - acc: 0.8625 - val_loss: 0.5238 - val_acc: 0.7337\n",
      "Epoch 8/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.2257 - acc: 0.9187Epoch 00007: loss improved from 0.34724 to 0.23809, saving model to weights-imdb-bilstm-07-0.2381.hdf5\n",
      "800/800 [==============================] - 59s - loss: 0.2381 - acc: 0.9100 - val_loss: 0.6336 - val_acc: 0.6884\n",
      "Epoch 9/20\n",
      "750/800 [===========================>..] - ETA: 2s - loss: 0.1690 - acc: 0.9387Epoch 00008: loss improved from 0.23809 to 0.16554, saving model to weights-imdb-bilstm-08-0.1655.hdf5\n",
      "800/800 [==============================] - 50s - loss: 0.1655 - acc: 0.9413 - val_loss: 0.5247 - val_acc: 0.7688\n",
      "Epoch 10/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.0649 - acc: 0.9867Epoch 00009: loss improved from 0.16554 to 0.06459, saving model to weights-imdb-bilstm-09-0.0646.hdf5\n",
      "800/800 [==============================] - 51s - loss: 0.0646 - acc: 0.9863 - val_loss: 0.7079 - val_acc: 0.7487\n",
      "Epoch 11/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.0638 - acc: 0.9800Epoch 00010: loss improved from 0.06459 to 0.06293, saving model to weights-imdb-bilstm-10-0.0629.hdf5\n",
      "800/800 [==============================] - 50s - loss: 0.0629 - acc: 0.9800 - val_loss: 0.6115 - val_acc: 0.7688\n",
      "Epoch 12/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.0825 - acc: 0.9680Epoch 00011: loss did not improve\n",
      "800/800 [==============================] - 51s - loss: 0.0781 - acc: 0.9700 - val_loss: 0.6228 - val_acc: 0.7538\n",
      "Epoch 13/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.0083 - acc: 1.0000Epoch 00012: loss improved from 0.06293 to 0.00864, saving model to weights-imdb-bilstm-12-0.0086.hdf5\n",
      "800/800 [==============================] - 51s - loss: 0.0086 - acc: 1.0000 - val_loss: 0.6604 - val_acc: 0.7688\n",
      "Epoch 14/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.0035 - acc: 1.0000Epoch 00013: loss improved from 0.00864 to 0.00333, saving model to weights-imdb-bilstm-13-0.0033.hdf5\n",
      "800/800 [==============================] - 51s - loss: 0.0033 - acc: 1.0000 - val_loss: 0.7300 - val_acc: 0.7739\n",
      "Epoch 15/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.0011 - acc: 1.0000Epoch 00014: loss improved from 0.00333 to 0.00128, saving model to weights-imdb-bilstm-14-0.0013.hdf5\n",
      "800/800 [==============================] - 52s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7890 - val_acc: 0.7789\n",
      "Epoch 16/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 4.4609e-04 - acc: 1.0000Epoch 00015: loss improved from 0.00128 to 0.00044, saving model to weights-imdb-bilstm-15-0.0004.hdf5\n",
      "800/800 [==============================] - 55s - loss: 4.4500e-04 - acc: 1.0000 - val_loss: 0.9555 - val_acc: 0.7387\n",
      "Epoch 17/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 1.5175e-04 - acc: 1.0000Epoch 00016: loss improved from 0.00044 to 0.00015, saving model to weights-imdb-bilstm-16-0.0001.hdf5\n",
      "800/800 [==============================] - 59s - loss: 1.4853e-04 - acc: 1.0000 - val_loss: 0.9592 - val_acc: 0.7638\n",
      "Epoch 18/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 6.0707e-05 - acc: 1.0000Epoch 00017: loss improved from 0.00015 to 0.00006, saving model to weights-imdb-bilstm-17-0.0001.hdf5\n",
      "800/800 [==============================] - 52s - loss: 5.8096e-05 - acc: 1.0000 - val_loss: 1.0801 - val_acc: 0.7538\n",
      "Epoch 19/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 2.4832e-05 - acc: 1.0000Epoch 00018: loss improved from 0.00006 to 0.00003, saving model to weights-imdb-bilstm-18-0.0000.hdf5\n",
      "800/800 [==============================] - 54s - loss: 2.6585e-05 - acc: 1.0000 - val_loss: 1.4005 - val_acc: 0.7337\n",
      "Epoch 20/20\n",
      "750/800 [===========================>..] - ETA: 3s - loss: 0.2367 - acc: 0.9707Epoch 00019: loss did not improve\n",
      "800/800 [==============================] - 52s - loss: 0.2219 - acc: 0.9725 - val_loss: 0.8823 - val_acc: 0.7186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd6f84fdd50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(GRU(100))(embedded_sequences)\n",
    "preds = Dense(2, activation='softmax')(l_lstm)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "model.summary()\n",
    "filepath=\"weights-imdb-bilstm-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=2, batch_size=50, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
